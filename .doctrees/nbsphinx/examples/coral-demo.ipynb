{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f88ba99",
   "metadata": {},
   "source": [
    "# Using the NengoEdge Coral Runner\n",
    "\n",
    "In this example we will walk through loading and running a model exported from NengoEdge\n",
    "that's been uniquely configured to run on Coral's edge TPU dev board. The goal of this\n",
    "demo is to provide a template for you to make your own custom TPU accelerated\n",
    "applications using the NengoEdge runner."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b36a7fbb",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, install NengoEdge tools using\n",
    "[these instructions](https://www.nengo.ai/nengo-edge/developers.html).\n",
    "Feel free to skip the Tensorflow step since we won't need it for this\n",
    "runner.\n",
    "\n",
    "For the Coral example, it's assumed that you have completed the Coral.ai\n",
    "[getting started guide](https://coral.ai/docs/dev-board/get-started/) such that,\n",
    "\n",
    "- The `tflite_runtime` Python package is installed on the board\n",
    "- The Edge TPU delegates are installed (e.g.`libedgetpu.so.1` for Linux systems)\n",
    "- You are able to ssh into your local board (e.g `ssh mendel@<CORAL_HOSTNAME>`; see\n",
    "  [tutorial](https://coral.ai/docs/dev-board/mdt/#connect-using-other-ssh-tools) if not\n",
    "  yet set up)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38218e2a",
   "metadata": {},
   "source": [
    "## Train a model in NengoEdge\n",
    "\n",
    "The first step is to train a model in NengoEdge for the **Coral Dev Board** hardware.\n",
    "See\n",
    "[this blog\n",
    "post](https://appliedbrainresearch.com/blog/fast-keyword-detection-with-lmus-on-gpu)\n",
    "for a detailed walkthrough on how to train such a model.\n",
    "\n",
    "## Export the trained model\n",
    "\n",
    "When exporting the model from NengoEdge you must choose the \"BINARY\" option to get a\n",
    "model targeted to run on the Edge TPU. The downloaded artifacts can be unpacked to a\n",
    "directory of your choice, and for the purpose of this demo we'll assume the contents\n",
    "have been unpacked to a directory called `coral_demo/`.\n",
    "\n",
    "Inside this directory you'll find two files:\n",
    "\n",
    "- `model_edgetpu.tflite`\n",
    "- `parameters.json`\n",
    "\n",
    "We'll create a `CoralRunner` that utilizes these artifacts.\n",
    "\n",
    "Note that if you are running this code locally, you will need to uncomment the\n",
    "`os.environ` line and update `<CORAL_HOSTNAME>` to point to the SSH hostname\n",
    "(see the Installation steps above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "879ba6a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T13:32:40.345309Z",
     "iopub.status.busy": "2024-05-09T13:32:40.345083Z",
     "iopub.status.idle": "2024-05-09T13:32:43.091764Z",
     "shell.execute_reply": "2024-05-09T13:32:43.090876Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 13:32:40.722731: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-09 13:32:40.745291: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-09 13:32:40.746035: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 13:32:41.163824: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/runner/_work/nengo-edge/nengo-edge/nengo-edge/nengo_edge/config.py:37: UserWarning: Downloaded model uses nengo-edge 23.9.11.dev0, but you're using nengo-edge 24.5.2.dev0. Mismatch may cause errors.\n",
      "  warnings.warn(\n",
      "Warning: Permanently added '10.42.88.22' (ECDSA) to the list of known hosts.\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from nengo_edge import CoralRunner\n",
    "\n",
    "# os.environ[\"CORAL_HOSTNAME\"] = <CORAL_HOSTNAME>\n",
    "runner = CoralRunner(\n",
    "    directory=\"coral_demo\",\n",
    "    username=\"mendel\",\n",
    "    hostname=os.environ[\"CORAL_HOSTNAME\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b77fde9",
   "metadata": {},
   "source": [
    "You will likely have particular audio inputs you are interested in identifying with the\n",
    "edge key word spotting model, but we'll showcase the general run steps assuming a random\n",
    "signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10a42836",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T13:32:43.095479Z",
     "iopub.status.busy": "2024-05-09T13:32:43.094650Z",
     "iopub.status.idle": "2024-05-09T13:32:43.099505Z",
     "shell.execute_reply": "2024-05-09T13:32:43.098970Z"
    }
   },
   "outputs": [],
   "source": [
    "audio_inputs = np.random.uniform(\n",
    "    -1, 1, (1, runner.preprocessing[\"sample_rate\"])\n",
    ").astype(\"float32\")\n",
    "\n",
    "# Keyword labels for model outputs\n",
    "labels = [\n",
    "    \"<silence>\",\n",
    "    \"<unknown>\",\n",
    "    \"yes\",\n",
    "    \"no\",\n",
    "    \"up\",\n",
    "    \"down\",\n",
    "    \"left\",\n",
    "    \"right\",\n",
    "    \"on\",\n",
    "    \"off\",\n",
    "    \"stop\",\n",
    "    \"go\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f131ede6",
   "metadata": {},
   "source": [
    "And finally we can run the model using the Coral device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19a21676",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T13:32:43.101940Z",
     "iopub.status.busy": "2024-05-09T13:32:43.101567Z",
     "iopub.status.idle": "2024-05-09T13:32:45.009793Z",
     "shell.execute_reply": "2024-05-09T13:32:45.009151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted keyword: off\n"
     ]
    }
   ],
   "source": [
    "outputs = runner.run(audio_inputs)\n",
    "pred_label = np.argmax(outputs)\n",
    "\n",
    "print(f\"Predicted keyword: {labels[pred_label]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53be66c2",
   "metadata": {},
   "source": [
    "Since we used a random audio sample here and the model was trained on real audio\n",
    "samples, it's likely this particular classification will\n",
    "result in a random label. Note that the first `runner.run(inputs)` call will copy\n",
    "over necessary runtime files that persist for the entire session. Subsequent calls to\n",
    "run will only copy inputs and outputs to/from the Coral board. To reset this behaviour,\n",
    "run `runner.reset()`. You'll notice that in this example we only ran the model on a\n",
    "single batch of inputs but the runner also supports batched inputs.\n",
    "\n",
    "That's it! You're now set to take this runner into your own applications."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
